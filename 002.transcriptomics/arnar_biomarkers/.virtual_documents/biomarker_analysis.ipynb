import pandas, scipy, numpy, seaborn

import matplotlib, matplotlib.pyplot 
import sklearn, sklearn.cluster
import statsmodels, statsmodels.stats, statsmodels.stats.multitest


# read biomarkers
df = pandas.read_excel('Endothelialbiomarkers.xlsx')
print(df.shape)
biomarkers = list(set(df['Gene']))
print(len(biomarkers))

for element in biomarkers:
    the_list = list(df['Gene'])
    counter = the_list.count(element)
    if counter > 1:
        print(f'Element {element} is twice')





# read annotation
anno_file = '/Users/adrian/software/kallisto/human_index_standard/t2g.txt'
df = pandas.read_csv(anno_file, sep='\t', header=None)

sub = df[df[2].isin(biomarkers)]
match = list(set(sub[1]))
print(len(match))

ensembl_bm = [element.split('.')[0] for element in match]
ensembl_bm.sort()
print(ensembl_bm)


rosetta = {}
for index, row in sub.iterrows():
    gene_name = row[2]
    ensembl = row[1].split('.')[0]
    if gene_name not in rosetta.keys():
        rosetta[ensembl] = gene_name
print(len(rosetta))
print(rosetta)


# read expression profiles
inputfile = 'vigur Suppl. Info. Table SIT1 expression profiles.csv'
df = pandas.read_csv(inputfile, sep=',', skiprows=3, index_col='ENSEMBL ID')
df.drop(columns=['Gene name', 'Gene biotype'], inplace=True)
df = df.loc[ensembl_bm, :]
df = df.apply(pandas.to_numeric, errors='coerce')
print(df.shape)
df.head()


# changing to gene names
df = df.rename(index=rosetta)


# dropping TNF because is zero expressed
df.drop(labels='TNF', inplace=True)
df.shape


# testing significance
sample_groupings = [
    [
        ['RSS_HLMV_34', 'RSS_HLMV_36'],
        ['RSS_HLMV_37', 'RSS_HLMV_38', 'RSS_HLMV_39'],
        ['RSS_HLMV_41', 'RSS_HLMV_42']   
    ],
    [
        ['RSS_HLMV_46', 'RSS_HLMV_47', 'RSS_HLMV_48'],
        ['RSS_HLMV_49', 'RSS_HLMV_50', 'RSS_HLMV_51'],
        ['RSS_HLMV_52', 'RSS_HLMV_53', 'RSS_HLMV_54']
    ]
]
reference_groupings = [
    ['RSS_HLMV_31','RSS_HLMV_32', 'RSS_HLMV_33'],
    ['RSS_HLMV_43', 'RSS_HLMV_44', 'RSS_HLMV_45']
]

pvalues = []
for index, row in df.iterrows():
    #print(index)
    for i in range(2):
        for j in range(3):
            samples = df.loc[index, sample_groupings[i][j]].values
            references = df.loc[index, reference_groupings[i]].values
            [statistic, pvalue] = scipy.stats.ttest_ind(samples, references)
            if numpy.isnan(pvalue):
                print(samples, references, pvalue)
                print('converting to P = 1')
                pvalue = 1
            pvalues.append(pvalue)
            #print(sample_groupings[i][j], samples)
            #print(reference_groupings[i], references)
            #print(pvalue)
            #print()
    #print()

# multiple correction
print(pvalues[:5])
result = statsmodels.stats.multitest.multipletests(pvalues, method='fdr_bh')
print(len(result[0]))

times = ['T4', 'T24']; concentrations = ['half', 'five', 'fifty']
loc = 0
annot_dict = {}
for k in range(len(df.index)):
    for i in range(2):
        for j in range(3):
            loc = k*6 + i*3  + j
            if result[0][loc] == True:
                print(k, df.index[k], loc, times[i], concentrations[j], result[0][loc], result[1][loc])
                annot_dict[(df.index[k], times[i]+concentrations[j])] = '*' 


annot_dict


# collapse replicates to medians
initial_column_names = df.columns
print(initial_column_names)

df['T4zero'] = numpy.median(df[['RSS_HLMV_31','RSS_HLMV_32', 'RSS_HLMV_33']], axis=1)
df['T4half'] = numpy.median(df[['RSS_HLMV_34', 'RSS_HLMV_36']], axis=1)
df['T4five'] = numpy.median(df[['RSS_HLMV_37', 'RSS_HLMV_38', 'RSS_HLMV_39']], axis=1)
df['T4fifty'] = numpy.median(df[['RSS_HLMV_41', 'RSS_HLMV_42']], axis=1)

df['T24zero'] = numpy.median(df[['RSS_HLMV_43', 'RSS_HLMV_44', 'RSS_HLMV_45']], axis=1)
df['T24half'] = numpy.median(df[['RSS_HLMV_46', 'RSS_HLMV_47', 'RSS_HLMV_48']], axis=1)
df['T24five'] = numpy.median(df[['RSS_HLMV_49', 'RSS_HLMV_50', 'RSS_HLMV_51']], axis=1)
df['T24fifty'] = numpy.median(df[['RSS_HLMV_52', 'RSS_HLMV_53', 'RSS_HLMV_54']], axis=1)

df.drop(columns=initial_column_names, inplace=True)

print(df.shape)
df.head()


# exclude gene whose max difference is below 10 TPMs
excluding = []
for index, row in df.iterrows():
    floor = row.min()
    ceiling = row.max()
    difference = ceiling - floor
    rel_change = numpy.log2((ceiling+1)/(floor+1))
    
    if difference < 10:
        excluding.append(index)
        print(f'Excluding {index} because difference is {difference}')
    else:
        if rel_change < 1:
            print('\tWARNING', index, difference, numpy.log2(ceiling/floor))

df.drop(labels=excluding, inplace=True)
print(df.shape)
df.head()


zscore_df = df.apply(scipy.stats.zscore, axis=1)
zscore_df.head()


linkage_method = 'average'
distance_metric = 'cosine'

seaborn.clustermap(zscore_df, 
                   cmap='bwr', 
                   col_cluster=True, 
                   row_cluster=True,
                   vmin=-2, vmax=2, 
                   method=linkage_method, 
                   metric=distance_metric, 
                   yticklabels=1, 
                   cbar_kws={'label':'z-score'})

matplotlib.pyplot.show()


# determine the best partition
x = zscore_df.to_numpy()
k_range = range(2, 8+1)

pvx = []; pvy = []; all_labels = []
for k in k_range:

  kmeans_model = sklearn.cluster.AgglomerativeClustering(n_clusters=k, 
                                                         metric='cosine', linkage='average').fit(x)
  labels = kmeans_model.labels_
  
  ss = sklearn.metrics.silhouette_score(x, labels, metric='cosine') # best value is 1, worst is -1 ergo k=6
  khi = sklearn.metrics.calinski_harabasz_score(x, labels) # higher better ergo 2
  dbi = sklearn.metrics.davies_bouldin_score(x, labels) # lower better ergo 8

  print(k, ss, khi, dbi)
  pvx.append(k)
  pvy.append(ss)
  all_labels.append(labels)

best_partition_index = 4
worst_partition_index = 2

matplotlib.pyplot.plot(pvx, pvy, 'ok')
matplotlib.pyplot.plot(pvx, pvy, ':', color='black')
matplotlib.pyplot.plot([pvx[best_partition_index], pvx[best_partition_index]], [pvy[worst_partition_index], pvy[best_partition_index]], ls='--')

matplotlib.pyplot.grid(alpha=0.5, ls=':')
matplotlib.pyplot.xlabel('number of clusters (k)')
matplotlib.pyplot.ylabel('Silhoutte score')
matplotlib.pyplot.tight_layout()

groupings = all_labels[best_partition_index]
print(groupings)

color_options = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:gray']
k_colors = []
for i in range(len(zscore_df.index)):
  k_colors.append(color_options[groupings[i]])




annot = pandas.DataFrame('', index=zscore_df.index, columns=zscore_df.columns)
for (row, col), val in annot_dict.items():
    if row not in zscore_df.index:
        print('bypassing', row)
    else:
        annot.at[row, col] = val
annot


seaborn.set(font_scale=0.9)

seaborn.clustermap(zscore_df, 
                   annot=annot, fmt='', annot_kws={'va':'center_baseline', 'size':18},
                   cmap='bwr', 
                   col_cluster=True, 
                   row_cluster=True,
                   vmin=-2.5, vmax=2.5, 
                   method=linkage_method, 
                   metric=distance_metric, 
                   yticklabels=1, 
                   cbar_kws={'label':'z-score'}, 
                   row_colors=k_colors)

matplotlib.pyplot.tight_layout()
matplotlib.pyplot.savefig('/Users/adrian/scratch/heatmap.svg')


















